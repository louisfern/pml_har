---
title: "Practical Machine Learning - Human Activity Recognition"
author: "Louis Fernandes"
date: "February 27, 2016"
output: html_document
---

## Introduction

This project uses the freely available weight lifting activity recognition dataset from the Groupware group at the [Pontifícia Universidade Católica do Rio de Janeiro](http://groupware.les.inf.puc-rio.br/har). The goal as described on the Coursera Practical Machine Learning (website)[https://www.coursera.org/learn/practical-machine-learning/supplement/PvInj/course-project-instructions-read-first] is to predict the manner in which an exercise was performed. Briefly, the experimentalists equipped sensors to the arms, hands, waist, and dumbbell as six participants performed lightly weighted Unilateral Dumbbell Biceps Curls. The participants were coached to perform the motion in one of five different manners, corresponding to the correct form and four common error modes. The sensor data captured 96 total derived feature sets. For more information on the details of the data set and experimental protocol, see [1].

The goal of this project is to use the provided training data set to predict the class of a subset of the observations. 

## Data loading and cleaning

```{r loading, cache=TRUE}
training <- read.csv('./data/pml-training.csv')
testing <- read.csv('./data/pml-testing.csv')
set.seed(1984)
library(caret)
```

We notice upon inspection that some rows that have "yes" under the "new_window" variable seem to have different data than the other rows. As these rows have  summary statistics (avg, std, kurtosis, etc.), they are presumably the result of earlier data processing. These observations will be excluded, as the test set does not have both "yes" and "no" present. We will then remove blank and NA variables from the remaining datasets, as well as any variables with only one class. We will also remove the variable `cvtdtimestamp` as it is underrepresented in the testing set, which will cause classification issues with the random forest (see below). We also remove the label variable `X`.

```{r cleaning, cache=TRUE}
training <- training[training$new_window=="no",]
testing <- testing[testing$new_window=="no",]

isNA <- apply(is.na(training), 2, any)
training[,isNA] <- list(NULL)
testing[,isNA] <- list(NULL)

training <- training[, sapply(training, function(col) length(unique(col))) > 1]
testing <- testing[, sapply(testing, function(col) length(unique(col))) > 1]

isFactor <- sapply(training, function(col) is.factor(col))
training[!isFactor] <- lapply(training[!isFactor], function(x) as.numeric(x))
testing[!isFactor] <- lapply(testing[!isFactor], function(x) as.numeric(x))

training[,"cvtd_timestamp"] <- NULL
testing[,"cvtd_timestamp"] <- NULL

training[,"X"] <- NULL
testing[,"X"] <- NULL
```

We are also concerned with extreme outliers in our data. Ideally, we have outside information regarding the valid ranges of our data that could inform a filter that would remove outliers. Unfortunately, due to the poor quality of the data coming in, we don't have such. We will use a very crude method to detect outliers: we will throw out any measurement that is >6 standard deviations from the column mean. 

```{r removeOutliers, cache=TRUE}

## This function takes in a column and returns a logical array of rows that
## exceed the threshold of 6 sigma
outlier <- function(x) {
  outlier <- abs(x-mean(x, na.rm=TRUE)) > 6*sd(x, na.rm=TRUE)
}

## We iterate over every variable and mark the rows as outliers, except for the factor variables! Those get a pass.
n <- names(training)
training$outlier<-FALSE
for (i in 2:56){
  out <- outlier(training[,n[i]])
  training$outlier[out==TRUE] <- TRUE
}

numOutliers <- sum(training$outlier)

## Drop the outliers from the training set and remove the boolean column
training <- training[!training$outlier,]
training <- training[,-which(names(training)=="outlier")]


```

We found `r numOutliers` outliers using our crude approach. Removing these should improve our classification.

## Training 

### Validation set

First we will separate out a fraction of our training set as a validation set. We will use this set to test the result of our classifier.

```{r partition, cache=TRUE}
tr <- createDataPartition(training$classe, p=0.75, list=FALSE)
train <- training[tr,]
valid <- training[-tr,]
```

### Pre-processing

Next, we will preprocess our data to scale, center, and reduce its dimensionality with PCA. 

```{r preprocess, cache=TRUE}
preProc <- preProcess(x = train[,-57], method = c("center","scale","pca"), thresh=0.85)
trainPreProcPred <- predict(preProc, train[,-57])
```

### Training

Calling `summary` on 'train' reveals many factors with large spread. We presume here that the data is fairly noisy, and will accordingly begin our training with a random forest, which should perform better in a noisy environment than bagging [2]. We will train with 10-fold, 5 repeat cross-validation. 

```{r training, cache=TRUE}

f3 <- trainControl(method="repeatedcv", number = 10, repeats=5, verboseIter = FALSE)
rfModel <- train(trainPreProcPred, train$classe,
                  method = "rf", trControl = f3)

```

## Results

### Validation set

Now that we have our training model, we will assess our performance on our validation set. 

```{r validation, cache=TRUE}
validPreProcPred <- predict(preProc, valid)
validPred<-predict(rfModel, validPreProcPred)
confusionMatrix(validPred, valid$classe)
```

Our classifier gets all but a few observations in the validation set correct. The out of sample error is represented here by the accuracy of our classifier as displayed in the command `print(rfModel)`:

```{r accuracy, cache=TRUE}
print(rfModel)
```

Our accuracy is ~96%, and this is for `mtry = 2`. This accuracy reported here is the out of bag accuracy of a random forest, which is the same as the out of sample error [2]. 

```{r plot1, cache=TRUE}
ggplot(rfModel) + theme_bw() + ggtitle("Accuracy of model as function of Predictors")
```

Here we see  that our accuracy drops as `mtry` increases. My assumption here is that as we add more fitting parameters to our data, we begin to train too much to the noise of the system. Further exploration of this parameter would be interesting.

### Final test 

We then tested on the original test set, minus the labels and timestamps, and got 20/20 predictions correct. This is quite good! Indeed, it's the best you could do. 

There is further room for exploration in this project. Further reduction of features, other classifiers, and further tweaks to the parameters of the random forest would all be places to explore next. Accuracy improvement would be hard (impossible) to do, but run time could be greatly improved.

## Software information

### Packages used
`caret`

### Version information
```{r version, cache=TRUE}
Sys.info()
version
packageVersion("caret")

```



## References

[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
Read more: http://groupware.les.inf.puc-rio.br/har#ixzz41OnSGqmm

[2] Robnik-Sikonja, M. Improving Random Forests. In J.F. Boulicat et. al (eds.): Machine Learning, ECML 2004 Proceedings, Springer, Berlin, 2004.